{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/final_train.csv')\n",
    "test_df = pd.read_csv('data/final_test.csv')\n",
    "test_id_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51635 entries, 0 to 51634\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   id            51635 non-null  int64\n",
      " 1   plate         51635 non-null  int64\n",
      " 2   date          51635 non-null  int64\n",
      " 3   price         51635 non-null  int64\n",
      " 4   forbidden     51635 non-null  int64\n",
      " 5   advantage     51635 non-null  int64\n",
      " 6   significance  51635 non-null  int64\n",
      " 7   city          51635 non-null  int64\n",
      "dtypes: int64(8)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = df.drop(columns=['price']),df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51635 entries, 0 to 51634\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   id            51635 non-null  int64\n",
      " 1   plate         51635 non-null  int64\n",
      " 2   date          51635 non-null  int64\n",
      " 3   forbidden     51635 non-null  int64\n",
      " 4   advantage     51635 non-null  int64\n",
      " 5   significance  51635 non-null  int64\n",
      " 6   city          51635 non-null  int64\n",
      "dtypes: int64(7)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Ensure same data type\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return 100 * tf.reduce_mean(2 * tf.abs(y_pred - y_true) / (tf.abs(y_true) + tf.abs(y_pred) + tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 381615.9375 - smape: 86.7791 - val_loss: 400768.4688 - val_smape: 81.6501\n",
      "Epoch 2/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 374676.1562 - smape: 80.0391 - val_loss: 403666.3125 - val_smape: 82.4980\n",
      "Epoch 3/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 367318.0000 - smape: 80.1688 - val_loss: 397684.0000 - val_smape: 80.6701\n",
      "Epoch 4/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 361800.8438 - smape: 79.7097 - val_loss: 397155.2188 - val_smape: 80.4946\n",
      "Epoch 5/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 374857.4375 - smape: 80.1477 - val_loss: 400242.3438 - val_smape: 81.4696\n",
      "Epoch 6/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 374152.5000 - smape: 79.9502 - val_loss: 400535.6875 - val_smape: 81.5603\n",
      "Epoch 7/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 367883.8125 - smape: 79.7572 - val_loss: 401477.9375 - val_smape: 81.8385\n",
      "Epoch 8/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 373961.7812 - smape: 80.5995 - val_loss: 399899.8750 - val_smape: 81.3660\n",
      "Epoch 9/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 371606.5625 - smape: 79.9853 - val_loss: 397522.2188 - val_smape: 80.6058\n",
      "Epoch 10/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 390882.7188 - smape: 80.2038 - val_loss: 401733.0312 - val_smape: 81.9123\n",
      "Epoch 11/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 367051.2500 - smape: 79.8276 - val_loss: 399075.8125 - val_smape: 81.1085\n",
      "Epoch 12/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 392188.7188 - smape: 80.6309 - val_loss: 397681.1875 - val_smape: 80.6688\n",
      "Epoch 13/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 373337.9375 - smape: 80.4649 - val_loss: 397688.7812 - val_smape: 80.6518\n",
      "Epoch 14/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 360964.0938 - smape: 79.9055 - val_loss: 404488.9062 - val_smape: 82.6750\n",
      "Epoch 15/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 377035.1875 - smape: 80.0418 - val_loss: 400368.9062 - val_smape: 81.5010\n",
      "Epoch 16/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 380666.1250 - smape: 79.9624 - val_loss: 400819.1562 - val_smape: 81.6451\n",
      "Epoch 17/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 386918.8438 - smape: 80.1779 - val_loss: 409161.5625 - val_smape: 83.9483\n",
      "Epoch 18/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 378246.9062 - smape: 80.2162 - val_loss: 399979.4688 - val_smape: 81.3912\n",
      "Epoch 19/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 368263.7500 - smape: 80.2027 - val_loss: 399338.6562 - val_smape: 81.1951\n",
      "Epoch 20/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 365382.5938 - smape: 80.0357 - val_loss: 407658.8438 - val_smape: 83.5750\n",
      "Epoch 21/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 372021.2812 - smape: 79.9544 - val_loss: 399639.2812 - val_smape: 81.2808\n",
      "Epoch 22/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 367084.1250 - smape: 79.6935 - val_loss: 397775.5938 - val_smape: 80.6966\n",
      "Epoch 23/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 367445.4062 - smape: 79.8930 - val_loss: 398159.1875 - val_smape: 80.8267\n",
      "Epoch 24/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 372636.3750 - smape: 80.0286 - val_loss: 400474.2188 - val_smape: 81.5224\n",
      "Epoch 25/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 364592.3750 - smape: 79.8092 - val_loss: 403515.3750 - val_smape: 82.4325\n",
      "Epoch 26/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 374741.8125 - smape: 80.1957 - val_loss: 401634.9062 - val_smape: 81.8949\n",
      "Epoch 27/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 363888.6875 - smape: 79.8412 - val_loss: 403975.2500 - val_smape: 82.5834\n",
      "Epoch 28/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 364461.8750 - smape: 80.1815 - val_loss: 398459.4062 - val_smape: 80.9128\n",
      "Epoch 29/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 366155.0938 - smape: 79.9488 - val_loss: 399980.3438 - val_smape: 81.3858\n",
      "Epoch 30/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 368087.5938 - smape: 80.0191 - val_loss: 408932.8750 - val_smape: 83.8885\n",
      "Epoch 31/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 372291.4375 - smape: 79.9268 - val_loss: 400729.0312 - val_smape: 81.6424\n",
      "Epoch 32/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 361747.8750 - smape: 80.0342 - val_loss: 400693.4688 - val_smape: 81.6436\n",
      "Epoch 33/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 381804.5000 - smape: 79.7020 - val_loss: 400154.1875 - val_smape: 81.4727\n",
      "Epoch 34/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 387648.5938 - smape: 79.9407 - val_loss: 398174.5312 - val_smape: 80.8386\n",
      "Epoch 35/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 372221.3438 - smape: 80.0077 - val_loss: 400975.9062 - val_smape: 81.7146\n",
      "Epoch 36/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 369766.9375 - smape: 79.9149 - val_loss: 402121.6562 - val_smape: 82.0805\n",
      "Epoch 37/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 372188.4688 - smape: 79.8991 - val_loss: 400383.3125 - val_smape: 81.5552\n",
      "Epoch 38/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 368415.9375 - smape: 80.1723 - val_loss: 405486.4062 - val_smape: 83.0254\n",
      "Epoch 39/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 385932.5938 - smape: 80.2169 - val_loss: 398074.3438 - val_smape: 80.8163\n",
      "Epoch 40/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 376748.3125 - smape: 79.7846 - val_loss: 404348.1875 - val_smape: 82.7351\n",
      "Epoch 41/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 393796.6250 - smape: 79.9236 - val_loss: 394435.0938 - val_smape: 79.5336\n",
      "Epoch 42/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 368896.3438 - smape: 79.8379 - val_loss: 402488.9375 - val_smape: 82.2212\n",
      "Epoch 43/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 364529.7812 - smape: 79.8511 - val_loss: 408668.2812 - val_smape: 83.8738\n",
      "Epoch 44/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 365160.2500 - smape: 79.8384 - val_loss: 404952.6562 - val_smape: 82.9145\n",
      "Epoch 45/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 369501.5000 - smape: 80.0001 - val_loss: 402734.5625 - val_smape: 82.3182\n",
      "Epoch 46/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 381574.0625 - smape: 79.9297 - val_loss: 400982.9062 - val_smape: 81.7859\n",
      "Epoch 47/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 374960.6875 - smape: 79.5983 - val_loss: 398667.6562 - val_smape: 81.0934\n",
      "Epoch 48/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 363339.5625 - smape: 79.8196 - val_loss: 397204.4375 - val_smape: 80.6575\n",
      "Epoch 49/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 378376.8750 - smape: 79.9890 - val_loss: 398898.9688 - val_smape: 81.2029\n",
      "Epoch 50/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 365234.5625 - smape: 79.6579 - val_loss: 411002.5625 - val_smape: 84.4858\n",
      "Epoch 51/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 371517.1250 - smape: 80.1654 - val_loss: 403145.9062 - val_smape: 82.3912\n",
      "Epoch 52/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 377682.0625 - smape: 80.0457 - val_loss: 398929.4062 - val_smape: 81.1920\n",
      "Epoch 53/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 370452.4688 - smape: 80.1101 - val_loss: 399067.0938 - val_smape: 81.1880\n",
      "Epoch 54/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 366032.1562 - smape: 79.8516 - val_loss: 400058.4375 - val_smape: 81.4685\n",
      "Epoch 55/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 373890.5312 - smape: 79.7554 - val_loss: 401431.1562 - val_smape: 81.8936\n",
      "Epoch 56/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 373946.4062 - smape: 79.8924 - val_loss: 399349.1562 - val_smape: 81.2918\n",
      "Epoch 57/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 371159.5625 - smape: 79.7371 - val_loss: 397043.7500 - val_smape: 80.5844\n",
      "Epoch 58/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 367349.1562 - smape: 79.5344 - val_loss: 405623.6562 - val_smape: 83.1135\n",
      "Epoch 59/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 376926.7500 - smape: 79.5986 - val_loss: 401673.2812 - val_smape: 81.9424\n",
      "Epoch 60/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 358818.1875 - smape: 79.5098 - val_loss: 399412.8125 - val_smape: 81.1376\n",
      "Epoch 61/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 362989.0000 - smape: 80.5089 - val_loss: 395478.3125 - val_smape: 80.0867\n",
      "Epoch 62/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 384916.2500 - smape: 79.6540 - val_loss: 393561.7500 - val_smape: 79.3645\n",
      "Epoch 63/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 386544.2500 - smape: 79.7435 - val_loss: 400932.2812 - val_smape: 81.6550\n",
      "Epoch 64/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 369867.5000 - smape: 79.3627 - val_loss: 402797.9062 - val_smape: 82.3324\n",
      "Epoch 65/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 371545.1875 - smape: 79.9622 - val_loss: 411650.4375 - val_smape: 84.6182\n",
      "Epoch 66/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 370767.8125 - smape: 79.7275 - val_loss: 399226.2500 - val_smape: 81.3287\n",
      "Epoch 67/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 371703.2812 - smape: 79.5510 - val_loss: 397271.7188 - val_smape: 80.7316\n",
      "Epoch 68/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 369677.0000 - smape: 79.9331 - val_loss: 395380.2500 - val_smape: 79.9883\n",
      "Epoch 69/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 360179.3125 - smape: 79.6874 - val_loss: 400619.3125 - val_smape: 81.7288\n",
      "Epoch 70/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 372302.0000 - smape: 79.5414 - val_loss: 403320.3125 - val_smape: 82.4078\n",
      "Epoch 71/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 375430.1562 - smape: 79.3855 - val_loss: 397460.9062 - val_smape: 80.7515\n",
      "Epoch 72/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 363193.0312 - smape: 79.7056 - val_loss: 397850.9375 - val_smape: 80.8885\n",
      "Epoch 73/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 372275.5312 - smape: 80.0056 - val_loss: 393875.1562 - val_smape: 79.4934\n",
      "Epoch 74/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 384515.5938 - smape: 79.8493 - val_loss: 405097.0000 - val_smape: 82.9141\n",
      "Epoch 75/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 364458.4688 - smape: 79.2868 - val_loss: 408197.6250 - val_smape: 83.7155\n",
      "Epoch 76/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 377298.3125 - smape: 79.5531 - val_loss: 399851.2812 - val_smape: 81.4753\n",
      "Epoch 77/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 365345.5312 - smape: 79.4715 - val_loss: 405534.6875 - val_smape: 83.0843\n",
      "Epoch 78/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 364750.2500 - smape: 79.7390 - val_loss: 403556.6562 - val_smape: 82.5058\n",
      "Epoch 79/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 359763.0938 - smape: 79.7136 - val_loss: 397034.6250 - val_smape: 80.5484\n",
      "Epoch 80/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 370997.1875 - smape: 79.6924 - val_loss: 398820.5938 - val_smape: 81.1537\n",
      "Epoch 81/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 374025.2812 - smape: 79.7263 - val_loss: 396343.7812 - val_smape: 80.4196\n",
      "Epoch 82/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 373343.5938 - smape: 79.5834 - val_loss: 406783.1875 - val_smape: 83.4343\n",
      "Epoch 83/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 382673.0625 - smape: 79.6455 - val_loss: 401115.8750 - val_smape: 81.8303\n",
      "Epoch 84/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 365189.5312 - smape: 79.2722 - val_loss: 398829.1250 - val_smape: 81.0707\n",
      "Epoch 85/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 356871.0312 - smape: 79.5310 - val_loss: 397161.2188 - val_smape: 80.6638\n",
      "Epoch 86/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 365567.3125 - smape: 79.2884 - val_loss: 407130.0000 - val_smape: 83.5012\n",
      "Epoch 87/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 374200.7812 - smape: 79.4366 - val_loss: 405443.6250 - val_smape: 83.0523\n",
      "Epoch 88/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 379740.7188 - smape: 79.8832 - val_loss: 399655.5000 - val_smape: 81.4515\n",
      "Epoch 89/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 366566.9062 - smape: 79.4829 - val_loss: 393646.5000 - val_smape: 79.2546\n",
      "Epoch 90/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 369575.4062 - smape: 78.9989 - val_loss: 399170.9375 - val_smape: 81.1934\n",
      "Epoch 91/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 385356.3750 - smape: 79.9472 - val_loss: 402500.5938 - val_smape: 82.2389\n",
      "Epoch 92/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 384427.8125 - smape: 79.6793 - val_loss: 399301.8125 - val_smape: 81.2842\n",
      "Epoch 93/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 385049.5312 - smape: 79.3672 - val_loss: 399471.4375 - val_smape: 81.2297\n",
      "Epoch 94/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 366058.0625 - smape: 79.7547 - val_loss: 400697.3750 - val_smape: 81.5878\n",
      "Epoch 95/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 362468.9375 - smape: 80.0516 - val_loss: 396212.2812 - val_smape: 80.3962\n",
      "Epoch 96/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 381553.1250 - smape: 79.9230 - val_loss: 396531.8438 - val_smape: 80.3300\n",
      "Epoch 97/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 377933.3438 - smape: 79.9190 - val_loss: 403897.3125 - val_smape: 82.5378\n",
      "Epoch 98/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 380767.6875 - smape: 80.0610 - val_loss: 399553.5938 - val_smape: 81.3312\n",
      "Epoch 99/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 381682.5938 - smape: 80.1034 - val_loss: 394775.3438 - val_smape: 79.8943\n",
      "Epoch 100/100\n",
      "\u001b[1m4131/4131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 370125.7188 - smape: 79.6524 - val_loss: 403327.8750 - val_smape: 82.3603\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding layers\n",
    "classifier.add(Dense(units=50, kernel_initializer='he_uniform', activation='relu', input_dim=7))\n",
    "classifier.add(Dense(units=25, kernel_initializer='he_uniform', activation='relu'))\n",
    "classifier.add(Dense(units=50, kernel_initializer='he_uniform', activation='relu'))\n",
    "classifier.add(Dense(units=1, kernel_initializer='he_uniform', activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the ANN with a proper loss function\n",
    "classifier.compile(loss='mean_absolute_error', optimizer='Adamax', metrics=[smape])\n",
    "\n",
    "# Fit the model\n",
    "model_history = classifier.fit(X_train.values, y_train.values, validation_split=0.20, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df.drop(columns=['price']),df['price'],test_size = 0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m241/241\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step\n"
     ]
    }
   ],
   "source": [
    "ann_pred=classifier.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[171163.  ]\n",
      " [188090.48]\n",
      " [101714.31]\n",
      " ...\n",
      " [143949.7 ]\n",
      " [225557.33]\n",
      " [161756.95]]\n"
     ]
    }
   ],
   "source": [
    "print(ann_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(ann_pred)\n",
    "datasets=pd.concat([test_id_df['id'],pred],axis=1)\n",
    "datasets.columns=['id','price']\n",
    "datasets.to_csv('outputs/ann_sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
